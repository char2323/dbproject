import requests
from bs4 import BeautifulSoup
import json
import time
import re # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“

def scrape_douban_top250():
    """
    çˆ¬å–è±†ç“£ç”µå½± Top 250 çš„æ•°æ®ï¼Œå¹¶è¿›å…¥è¯¦æƒ…é¡µè·å–æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8' # æ˜ç¡®è¯·æ±‚ä¸­æ–‡å†…å®¹
    }
    
    all_movies = []
    
    for start_num in range(0, 250, 25):
        list_url = f'https://movie.douban.com/top250?start={start_num}&filter='
        print(f"æ­£åœ¨çˆ¬å–åˆ—è¡¨é¡µ (ç¬¬ {start_num // 25 + 1} é¡µ)...")
        
        try:
            response = requests.get(list_url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            movie_items = soup.find_all('div', class_='item')
            
            for item in movie_items:
                detail_url = item.find('div', class_='hd').find('a')['href']
                
                print(f"  -> æ­£åœ¨è¿›å…¥è¯¦æƒ…é¡µ...")
                try:
                    detail_response = requests.get(detail_url, headers=headers)
                    detail_response.raise_for_status()
                    detail_soup = BeautifulSoup(detail_response.text, 'lxml')

                    # --- ğŸ‘‡ å…¨æ–°çš„ã€æ›´å¯é çš„è§£æé€»è¾‘ ---
                    title = detail_soup.find('span', property='v:itemreviewed').get_text(strip=True)
                    cover_url = detail_soup.find('div', id='mainpic').find('img')['src']
                    
                    description_tag = detail_soup.find('span', property='v:summary')
                    description = description_tag.get_text(strip=True) if description_tag else "æš‚æ— ç®€ä»‹"

                    info_div = detail_soup.find('div', id='info')
                    
                    # æå–ä¸Šæ˜ æ—¥æœŸ
                    release_date_tag = info_div.find('span', string=re.compile(r'ä¸Šæ˜ æ—¥æœŸ'))
                    release_date = release_date_tag.next_sibling.strip() if release_date_tag else "1900-01-01"
                    
                    # æå–ç‰‡é•¿
                    duration_tag = info_div.find('span', string=re.compile(r'ç‰‡é•¿'))
                    duration_text = duration_tag.next_sibling.strip() if duration_tag else "0"
                    duration_match = re.search(r'\d+', duration_text)
                    duration_mins = int(duration_match.group(0)) if duration_match else 0
                    # --- ğŸ‘† è§£æé€»è¾‘ç»“æŸ ---

                    movie_data = {
                        "name": title,
                        "cover": cover_url,
                        "description": description,
                        "release_date": release_date.split('(')[0], # æ¸…ç†åœ°åŒºä¿¡æ¯
                        "duration_mins": duration_mins
                    }
                    all_movies.append(movie_data)
                    
                    time.sleep(1) 

                except requests.RequestException as e_detail:
                    print(f"    !! çˆ¬å–è¯¦æƒ…é¡µå¤±è´¥: {e_detail}")
                except Exception as e_parse:
                    print(f"    !! è§£æè¯¦æƒ…é¡µå¤±è´¥: {e_parse}, URL: {detail_url}")

        except requests.RequestException as e_list:
            print(f"!! çˆ¬å–åˆ—è¡¨é¡µå¤±è´¥: {e_list}")
            break
            
    final_data = {"movies": all_movies}
    
    try:
        with open('seed_data.json', 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        print(f"\næˆåŠŸï¼å…±çˆ¬å– {len(all_movies)} éƒ¨ç”µå½±çš„è¯¦ç»†æ•°æ®ï¼Œå·²å†™å…¥åˆ° seed_data.json æ–‡ä»¶ã€‚")
    except IOError as e_write:
        print(f"å†™å…¥æ–‡ä»¶å¤±è´¥: {e_write}")

if __name__ == '__main__':
    scrape_douban_top250()
